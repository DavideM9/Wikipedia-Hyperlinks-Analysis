{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Visit the Wikipedia hyperlinks graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we perform an analysis of the Wikipedia Hyperlink graph. In particular, given extra information about the\n",
    "categories to which an article belongs to, we are curious to rank the articles according to some criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RQ1] Build the graph G = (V, E), where V is the set of articles and E the hyperlinks among them, and provide its basic information:\n",
    "- If it is direct or not\n",
    "- The number of nodes\n",
    "- The number of edges\n",
    "- The average node degree. Is the graph dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "import math\n",
    "import statistics\n",
    "import heapq \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the file **wiki-topcats-reduced.txt**, we can notice the existence of a couple of nodes (107 e 104) for which it exists an edge from 107 to 104 and from 104 to 107. This make us realize that the graph is direct, otherwise, it would be useless the existence of one of the two edges. We thought about this proof: let's suppose that the graph is not direct. Then it can not exists a couple of nodes $a$ and $b$ such that exists an edge $a \\rightarrow b$ and an edge $b \\rightarrow a$. If exists a couple of nodes with this property than the graph is direct. To prove this we opened the file **reduced** and save it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = pd.read_csv('wiki-topcats-reduced.txt', sep = '\\t', names = ['source', 'destination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>401135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>1069112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>1163551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>12162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>167659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  destination\n",
       "0      52       401135\n",
       "1      52      1069112\n",
       "2      52      1163551\n",
       "3      62        12162\n",
       "4      62       167659"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645247"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we for each couple $(a,b)$ we add an edge $b\\rightarrow a$. In this way for each couple $(a,b)$ exists an edge $b\\rightarrow a$ and $a\\rightarrow b$. So if the are duplicates, then already exist a couple of nodes with this property, so the graph will be direct.\n",
    "To do this we invert the two columns, concatenating the two dataframe, and then we remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_inv=reduced.reindex(columns=['destination','source'])\n",
    "reduced_inv.columns=['source','destination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges of the graph with possible duplicates:  5290494\n",
      "Number of edges of the graph without duplicates:  4348125\n"
     ]
    }
   ],
   "source": [
    "print('Number of edges of the graph with possible duplicates: ',len(pd.concat([reduced,reduced_inv])))\n",
    "print('Number of edges of the graph without duplicates: ',len(pd.concat([reduced,reduced_inv]).drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of edges decrease. So the graph is direct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our graph, using a dictonary, in which the keys are the nodes and the values the list of nodes to which the node is linked by the edge. We also build the \"inverse_graph\", that we will need later. In the inverse graph the keys are the list of nodes and the values are the nodes from which the edge starts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2645247it [00:11, 236031.94it/s]\n"
     ]
    }
   ],
   "source": [
    "direct_graph=defaultdict(list)\n",
    "inverse_graph=defaultdict(list)\n",
    "with open('wiki-topcats-reduced.txt') as f: \n",
    "    for line in tqdm(f):\n",
    "        l=list(map(int,line.split()))\n",
    "        direct_graph[l[0]].append(l[1])\n",
    "        inverse_graph[l[1]].append(l[0])\n",
    "#complete the graph\n",
    "for key in inverse_graph.keys():\n",
    "    if key not in direct_graph:\n",
    "        direct_graph[key]=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the total number number of nodes, we make the union between the  set of the keys of the direct_graph and the set of the keys of the inverse_graph.\n",
    "The number of edge is the sum of the lengths of the lists in values of the direct graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  461193\n",
      "Number of edges:  2645247\n",
      "Average node degree (IN):  5.735661642739591\n",
      "Average node degree (OUT):  5.735661642739591\n",
      "Density ratio:  1.2436602635647606e-05\n"
     ]
    }
   ],
   "source": [
    "V=len(direct_graph.keys())\n",
    "E=sum(map(len,direct_graph.values()))\n",
    "D=E/(V*(V-1))\n",
    "print('Number of nodes: ',V)\n",
    "print('Number of edges: ',E)\n",
    "print('Average node degree (IN): ',sum(map(len,inverse_graph.values()))/V)\n",
    "print('Average node degree (OUT): ',E/V)\n",
    "print('Density ratio: ', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the in-degree and the density ratio we computed, we can say that the graph is not dense! Now we create another dictionary that has for keys the categories and values the list of articles that are in the category. After we will remove the categories that have less than 3500 argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories={}\n",
    "with open('wiki-topcats-categories.txt') as f:\n",
    "    for line in f:\n",
    "        l=line.split()\n",
    "        categories[l[0][9:-1]]=list(map(int,l[1::]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=categories.copy()\n",
    "for key in categories:\n",
    "    if len(categories[key])<=3500:\n",
    "        del(graph[key]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove the useless node, that are the nodes without outcoming edges and incoming edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cat=graph.copy()\n",
    "vertex=set(direct_graph.keys()).union(set(inverse_graph.keys()))\n",
    "for key in graph:\n",
    "    clean_cat[key]=list(set(graph[key]).intersection(vertex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add to the graph, the nodes that don't have outgoing edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RQ2] Given a category  $C_0 = \\{article_1, article_2, \\dots \\}$ as input we want to rank all of the nodes in V according to the following criteria: \n",
    "Obtain a block-ranking, where the blocks are represented by the categories. In particular, we want:\n",
    "\n",
    "$$block_{RANKING}=\\left[\\array{C_0,\\\\C_1,\\\\ \\cdots \\\\C_C }\\right]$$ \n",
    "\n",
    "Each category $C_i$ corresponds to a list of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chuncks of code, we implement some tools that we need for the analysis. The first tool is the BFS (Breadth-first-search), that we need for explore all the graph. Given a graph and a starting node (root) it returns the search_tree that is a tree that for each node has for sons, the closer nodes. We compute the distances between a node and the root during the bfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(graph, root):\n",
    "    seen, queue = set([root]), deque([root])\n",
    "    distances=defaultdict(lambda: math.inf)\n",
    "    distances[root] = 0\n",
    "    while queue:\n",
    "        vertex = queue.popleft()\n",
    "        d = distances[vertex]\n",
    "        for node in graph[vertex]:\n",
    "            if node not in seen:\n",
    "                seen.add(node)\n",
    "                queue.append(node)\n",
    "                distances[node] = d+1\n",
    "    return(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function *merge_distances* we can store all the shortest path from each node of the category $C_0$ to each nodes of each category $C_i$. At the end, we will get a dictionary where the keys are the nodes of the graph and the values are the lists of distances between the node and the nodes of $C_0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dist(d_min,d):\n",
    "    for key in d_min:\n",
    "        d_min[key].append(d[key])\n",
    "    return d_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to rank the categories using the following criteria: \n",
    "\n",
    "$$distance(C_0,C_i)=median(ShortestPath(C_0,C_i))$$\n",
    "\n",
    "we compute the median on the lists of lenghts of all shortest path from $C_0$ to $C_i \\quad \\forall i\\in\\{1,\\cdots,C\\}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_median(l):\n",
    "    return statistics.median(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2536/2536 [1:03:45<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "C0=clean_cat['Year_of_birth_unknown']\n",
    "min_dist={i:[] for i in direct_graph.keys()}\n",
    "for root in tqdm(C0):\n",
    "    dist=bfs(direct_graph, root)\n",
    "    min_dist=merge_dist(min_dist, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once calculated all length of all shortest path, we store it in a dictionary, build in this way:\n",
    "\n",
    "$$\\textit{short_path}=\\{C_i: [\\textit{distances between each nodes of $C_0$ and $C_i$}]\\}\\quad \\forall i \\in \\{0,\\cdots,C\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [12:22<00:00, 18.40s/it]\n"
     ]
    }
   ],
   "source": [
    "short_path=defaultdict(list)\n",
    "for cat in tqdm(clean_cat):\n",
    "    nodes=clean_cat[cat]\n",
    "    for key in nodes:\n",
    "        short_path[cat]+=min_dist[key] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we rank the categories using the value of the median. We store different tuples, made by:\n",
    "\n",
    "$$(\\textit{median}_i,C_i)$$\n",
    "\n",
    "in a min_heap. This because the min_heap has the minimum value of the root, so it's easy to sort this list of tuples, using the heapsort algorythm. We observe that there are a lot of nodes with distance equal to $\\infty$, this probabily because the graph is direct, so there are different nodes that have only outgoing edges, and also the graph is not dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [08:18<00:00,  2.10s/it]\n"
     ]
    }
   ],
   "source": [
    "ranking=[]\n",
    "for cat in tqdm(short_path):\n",
    "    heapq.heappush(ranking,(float(compute_median(short_path[cat])),cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(ranking)\n",
    "block_ranking=[]\n",
    "for  i in range(n):\n",
    "    block_ranking.append(heapq.heappop(ranking)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_ranking.remove('Year_of_birth_unknown')\n",
    "app=block_ranking\n",
    "block_ranking=['Year_of_birth_unknown']+ app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally this is our $block_{RANKING}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year_of_birth_unknown',\n",
       " 'American_film_actors',\n",
       " 'English-language_films',\n",
       " 'American_Jews',\n",
       " 'American_films',\n",
       " 'American_television_actors',\n",
       " 'Article_Feedback_Pilot',\n",
       " 'Black-and-white_films',\n",
       " 'British_films',\n",
       " 'English_television_actors',\n",
       " 'Indian_films',\n",
       " 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       " 'English-language_albums',\n",
       " 'People_from_New_York_City',\n",
       " 'Rivers_of_Romania',\n",
       " 'Debut_albums',\n",
       " 'American_military_personnel_of_World_War_II',\n",
       " 'Association_football_defenders',\n",
       " 'Association_football_forwards',\n",
       " 'Association_football_goalkeepers',\n",
       " 'Association_football_midfielders',\n",
       " 'Asteroids_named_for_people',\n",
       " 'English_cricketers',\n",
       " 'English_footballers',\n",
       " 'Fellows_of_the_Royal_Society',\n",
       " 'Harvard_University_alumni',\n",
       " 'Living_people',\n",
       " 'Main_Belt_asteroids',\n",
       " 'Major_League_Baseball_pitchers',\n",
       " 'Place_of_birth_missing_(living_people)',\n",
       " 'The_Football_League_players',\n",
       " 'Windows_games',\n",
       " 'Year_of_birth_missing',\n",
       " 'Year_of_birth_missing_(living_people)',\n",
       " 'Year_of_death_missing']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we have the $block_{RANKING}$ vector, we proceed with the sorting of nodes in each category:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Step 1]: Compute subgraph induced by $C_0$. For each node compute the sum of the weigths of the in-edges.\n",
    "\n",
    "$$score_{article_i}=\\sum_{i\\in in-edges} w_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the library *networkx* we compute the full graph, using the dictionary *direct_graph*\n",
    "after we use the same library for build the subgraph induced by the category $C_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.DiGraph(direct_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "C0=block_ranking[0]\n",
    "sg=G.subgraph(clean_cat[C0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(int)\n",
    "for i in list(sg.nodes):\n",
    "    score = sg.in_degree(i)\n",
    "    scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Step 2]:  Extend the graph to the nodes that belong to $C_1$. Thus, for each article in $C_1$ compute the score as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1=block_ranking[1]\n",
    "sg2=G.subgraph(clean_cat[C0] + clean_cat[C1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(sg2.nodes)-set(sg.nodes):\n",
    "    for j in range(len(list(sg2.in_edges(i)))):\n",
    "        sv = list(sg2.in_edges(i))[j][0]\n",
    "        if sv in list(sg.nodes):\n",
    "            scores[i] += scores[sv]\n",
    "        else:\n",
    "            scores[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Step 3]: Repeat Step2 up to the last category of the ranking. In the last step of the example you clearly see the weight update of the edge coming from node E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interessed to rank the articles in each categories. For sorting the articles using the score, we build the following function, that given a dictionary and a set of keys, it return the list of keys sorted by the value associated to each key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_ranking(d,s):\n",
    "    l=[]\n",
    "    for k in s:\n",
    "        l.append((k, d[k]))\n",
    "    sorted_by_second=sorted(l, key=lambda tup: tup[1])\n",
    "    l_sort=[x[0] for x in sorted_by_second]\n",
    "    \n",
    "    return(l_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:23<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_c = set()\n",
    "scores_tot = defaultdict(int)\n",
    "pages=[]\n",
    "for cat in tqdm(block_ranking):\n",
    "    set_clean_cat = set(clean_cat[cat])\n",
    "    considering= set_clean_cat-cat_c\n",
    "    cat_c = cat_c | set_clean_cat\n",
    "    sg_tot = G.subgraph(cat_c)\n",
    "    for i in (considering):\n",
    "        for j in (list(sg_tot.in_edges(i))):\n",
    "            sv = j[0]\n",
    "            if sv in considering:\n",
    "                scores_tot[i] += 1\n",
    "            else:\n",
    "                scores_tot[i] += scores_tot[sv]\n",
    "    pages+=page_ranking(scores_tot, set_clean_cat)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a nice visualization, we want to print the names of the articles, instead of the number of node. So we build the following dictionary, called *names* that has for keys the ID's of the article and for value the name of the ariticle:\n",
    "\n",
    "$$\\textit{names}=\\{\\textit{ArticleId}_i:\\textit{ArticleName}_i\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "names={}\n",
    "with open('wiki-topcats-page-names.txt') as f: \n",
    "    for line in f:\n",
    "        l=list(line.split())\n",
    "        names[int(l[0])]=' '.join(l[1::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the first 100 articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kisen\n",
      "Mibu no Tadami\n",
      "Jean Carol\n",
      "Conrad (Bishop of Utrecht)\n",
      "Anna Lehr\n",
      "Hodo Nivica\n",
      "Godfrey I, Duke of Lower Lorraine\n",
      "Hierocles (author of Synecdemus)\n",
      "Dahteste\n",
      "Manuel Surez de Begoa\n",
      "William Rainborowe\n",
      "Nikos E. Politis\n",
      "Mary Oxlie\n",
      "William Fraser (bishop)\n",
      "Elizabeth Stewart, Countess of Crawford\n",
      "Walter O'Hara\n",
      "Thomas Burton (politician)\n",
      "Archibald Van Horne\n",
      "Mila Schn\n",
      "Kzim-i-Samandar\n",
      "An Collins\n",
      "Mary Hill, Countess of Hillsborough\n",
      "Violet Mond, Baroness Melchett\n",
      "Raphael Neale\n",
      "Chevalier des Marchais\n",
      "Heather Lechtman\n",
      "Pemmasani Ramalinga Nayudu\n",
      "Varq\n",
      "Shivappa Nayaka\n",
      "Nicomachus (scribe)\n",
      "Alexander Wilson (U.S. Representative)\n",
      "Johan Danckerts\n",
      "James Johnson (Virginia congressman)\n",
      "Cornelio Da Montalcino\n",
      "William N. Grover\n",
      "Piper Kerman\n",
      "Thomas Davenport (congressman)\n",
      "Caroline Cooke\n",
      "Martin Parker\n",
      "Jean-Marie Boisvert\n",
      "Birgitte Macmillan, Countess of Stockton\n",
      "Charles Constantine of Vienne\n",
      "Francis Burleigh\n",
      "Agrippa the Skeptic\n",
      "Hilde Holovsky\n",
      "John Cochran (artist)\n",
      "Edith Borella\n",
      "Marcus Manilius\n",
      "Benjamin Avery\n",
      "Julius Frankenburg\n",
      "Cassius Apronianus\n",
      "Wynkyn de Worde\n",
      "Rogers Brubaker\n",
      "Fulvia (wife of Saturninus)\n",
      "Dharmapala (emperor)\n",
      "Isabella Whitney\n",
      "Henry Swearingen\n",
      "Jean Tijou\n",
      "Abdelkader Mokhtari\n",
      "Elizabeth Boyle, Countess of Guilford\n",
      "Richard Onslow (British Army officer)\n",
      "James Jones (Georgia)\n",
      "Richard of Pudlicott\n",
      "Gallus Mag\n",
      "Boston King\n",
      "Nathaniel Mist\n",
      "Mary Hignett\n",
      "Lawrence Chenault\n",
      "Lucius Caesonius Ovinius Rufinus Manilius Bassus\n",
      "Mel su I, Earl of Strathearn\n",
      "Edward Shepherd\n",
      "Carrie Fountain\n",
      "Gioacchino Cocchi\n",
      "Iio Noritsura\n",
      "Fred Pralle\n",
      "Minene Sakurano\n",
      "Greg Guy\n",
      "Lusius Quietus\n",
      "Margaret McMurray\n",
      "Buckshot Roberts\n",
      "Hugh Boscawen, 1st Viscount Falmouth\n",
      "Khai'r Bey\n",
      "Thomas Ady\n",
      "Kim Bracey\n",
      "Julius Briganticus\n",
      "Morishige\n",
      "Steve Hallam\n",
      "Jane Wiseman\n",
      "Chetan (actor)\n",
      "Erik Gnupsson\n",
      "John Hoskins (painter)\n",
      "Bill \"Kaipo\" Asing\n",
      "Isobel Gowdie\n",
      "Al Lee\n",
      "Chas S. Clifton\n",
      "Zoroaster\n",
      "Ninian E. Whiteside\n",
      "Antipope Christopher\n",
      "Nadir Mirza of Khorasan\n",
      "Ebrahim Afshar\n",
      "John of Seville\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "    print(names[pages[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

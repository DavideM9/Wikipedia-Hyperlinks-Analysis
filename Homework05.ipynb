{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Visit the Wikipedia hyperlinks graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we perform an analysis of the Wikipedia Hyperlink graph. In particular, given extra information about the\n",
    "categories to which an article belongs to, we are curious to rank the articles according to some criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RQ1] Build the graph G = (V, E), where V is the set of articles and E the hyperlinks among them, and provide its basic information:\n",
    "- If it is direct or not\n",
    "- The number of nodes\n",
    "- The number of edges\n",
    "- The average node degree. Is the graph dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "import math\n",
    "import statistics\n",
    "import heapq \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the file **wiki-topcats-reduced.txt**, we can notice the existence of a couple of nodes (107 e 104) for which it exists an edge from 107 to 104 and from 104 to 107. This make us realize that the graph is direct, otherwise, it would be useless the existence of one of the two edges. We thought about this proof: let's suppose that the graph is not direct. Then it can not exists a couple of nodes $a$ and $b$ such that exists an edge $a \\rightarrow b$ and an edge $b \\rightarrow a$. If exists a couple of nodes with this property than the graph is direct. To prove this we opened the file **reduced** and save it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = pd.read_csv('wiki-topcats-reduced.txt', sep = '\\t', names = ['source', 'destination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>401135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>1069112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>1163551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>12162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>167659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  destination\n",
       "0      52       401135\n",
       "1      52      1069112\n",
       "2      52      1163551\n",
       "3      62        12162\n",
       "4      62       167659"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645247"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we for each couple $(a,b)$ we add an edge $b\\rightarrow a$. In this way for each couple $(a,b)$ exists an edge $b\\rightarrow a$ and $a\\rightarrow b$. So if the are duplicates, then already exist a couple of nodes with this property, so the graph will be direct.\n",
    "To do this we invert the two columns, concatenating the two dataframe, and then we remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_inv=reduced.reindex(columns=['destination','source'])\n",
    "reduced_inv.columns=['source','destination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges of the graph with possible duplicates:  5290494\n",
      "Number of edges of the graph without duplicates:  4348125\n"
     ]
    }
   ],
   "source": [
    "print('Number of edges of the graph with possible duplicates: ',len(pd.concat([reduced,reduced_inv])))\n",
    "print('Number of edges of the graph without duplicates: ',len(pd.concat([reduced,reduced_inv]).drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of edges decrease. So the graph is direct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our graph, using a dictonary, in which the keys are the nodes and the values the list of nodes to which the node is linked by the edge. We also build the \"inverse_graph\", that we will need later. In the inverse graph the keys are the list of nodes and the values are the nodes from which the edge starts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2645247it [00:10, 260346.94it/s]\n"
     ]
    }
   ],
   "source": [
    "direct_graph=defaultdict(list)\n",
    "inverse_graph=defaultdict(list)\n",
    "with open('wiki-topcats-reduced.txt') as f: \n",
    "    for line in tqdm(f):\n",
    "        l=list(map(int,line.split()))\n",
    "        direct_graph[l[0]].append(l[1])\n",
    "        inverse_graph[l[1]].append(l[0])\n",
    "#complete the graph\n",
    "for key in inverse_graph.keys():\n",
    "    if key not in direct_graph:\n",
    "        direct_graph[key]=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the total number number of nodes, we make the union between the  set of the keys of the direct_graph and the set of the keys of the inverse_graph.\n",
    "The number of edge is the sum of the lengths of the lists in values of the direct graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  461193\n",
      "Number of edges:  2645247\n",
      "Average node degree (IN):  5.735661642739591\n",
      "Average node degree (OUT):  5.735661642739591\n",
      "Density ratio:  1.2436602635647606e-05\n"
     ]
    }
   ],
   "source": [
    "V=len(direct_graph.keys())\n",
    "E=sum(map(len,direct_graph.values()))\n",
    "D=E/(V*(V-1))\n",
    "print('Number of nodes: ',V)\n",
    "print('Number of edges: ',E)\n",
    "print('Average node degree (IN): ',sum(map(len,inverse_graph.values()))/V)\n",
    "print('Average node degree (OUT): ',E/V)\n",
    "print('Density ratio: ', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the in-degree and the density ratio we computed, we can say that the graph is not dense! Now we create another dictionary that has for keys the categories and values the list of articles that are in the category. After we will remove the categories that have less than 3500 argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories={}\n",
    "with open('wiki-topcats-categories.txt') as f:\n",
    "    for line in f:\n",
    "        l=line.split()\n",
    "        categories[l[0][9:-1]]=list(map(int,l[1::]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=categories.copy()\n",
    "for key in categories:\n",
    "    if len(categories[key])<=3500:\n",
    "        del(graph[key]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove the useless node, that are the nodes without outcoming edges and incoming edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cat=graph.copy()\n",
    "vertex=set(direct_graph.keys()).union(set(inverse_graph.keys()))\n",
    "for key in graph:\n",
    "    clean_cat[key]=list(set(graph[key]).intersection(vertex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add to the graph, the nodes that don't have outgoing edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RQ2] Given a category  $C_0 = \\{article_1, article_2, \\dots \\}$ as input we want to rank all of the nodes in V according to the following criteria: \n",
    "Obtain a block-ranking, where the blocks are represented by the categories. In particular, we want:\n",
    "\n",
    "$$block_{RANKING}=\\left[\\array{C_0,\\\\C_1,\\\\ \\cdots \\\\C_C }\\right]$$ \n",
    "\n",
    "Each category $C_i$ corresponds to a list of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chuncks of code, we implement some tools that we need for the analysis. The first tool is the BFS (Breadth-first-search), that we need for explore all the graph. Given a graph and a starting node (root) it returns the search_tree that is a tree that for each node has for sons, the closer nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(graph, root):\n",
    "    seen, queue = set([root]), deque([root])\n",
    "    distances=defaultdict(lambda: math.inf)\n",
    "    distances[root] = 0\n",
    "    while queue:\n",
    "        vertex = queue.popleft()\n",
    "        d = distances[vertex]\n",
    "        for node in graph[vertex]:\n",
    "            if node not in seen:\n",
    "                seen.add(node)\n",
    "                queue.append(node)\n",
    "                distances[node] = d+1\n",
    "    return(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the search_tree, we compute the distances between the root and each node using the following recursive function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dist(d_min,d):\n",
    "    for key in d_min:\n",
    "        d_min[key].append(d[key])\n",
    "    return d_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_median(l):\n",
    "    c=[x for x in l if x!=math.inf]\n",
    "    n_inf=l.count(math.inf)\n",
    "    return statistics.median(c)/math.log(n_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2536/2536 [1:07:51<00:00,  1.59it/s]  \n"
     ]
    }
   ],
   "source": [
    "C0=clean_cat['Year_of_birth_unknown']\n",
    "min_dist={i:[] for i in direct_graph.keys()}\n",
    "for root in tqdm(C0):\n",
    "    dist=bfs(direct_graph, root)\n",
    "    min_dist=merge_dist(min_dist, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('distances.json', 'w') as fp:\n",
    "    json.dump(min_dist, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [11:07<00:00, 16.58s/it]\n"
     ]
    }
   ],
   "source": [
    "short_path=defaultdict(list)\n",
    "for cat in tqdm(clean_cat):\n",
    "    nodes=clean_cat[cat]\n",
    "    for key in nodes:\n",
    "        short_path[cat]+=min_dist[key] #.append(min_dist[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [10:14<00:00,  3.30s/it]  \n"
     ]
    }
   ],
   "source": [
    "ranking=[]\n",
    "for cat in tqdm(short_path):\n",
    "    heapq.heappush(ranking,(float(compute_median(short_path[cat])),cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.350766055217393, 'Living_people'),\n",
       " (0.3595978887745843, 'English-language_films'),\n",
       " (0.37091218182629093, 'American_film_actors'),\n",
       " (0.3882433810587469, 'American_military_personnel_of_World_War_II'),\n",
       " (0.3688198550787606, 'American_films'),\n",
       " (0.38744280712432094, 'People_from_New_York_City'),\n",
       " (0.3728531191284263, 'American_television_actors'),\n",
       " (0.39593849257133024, 'Article_Feedback_Pilot'),\n",
       " (0.4482138406654756, 'Association_football_goalkeepers'),\n",
       " (0.40389049910919095, 'English_television_actors'),\n",
       " (0.3786004143060232, 'Harvard_University_alumni'),\n",
       " (0.4000412099261835, 'Year_of_birth_missing_(living_people)'),\n",
       " (0.3919424891362564, 'Fellows_of_the_Royal_Society'),\n",
       " (0.43284885756703656, 'The_Football_League_players'),\n",
       " (0.3732793895257392, 'Black-and-white_films'),\n",
       " (0.4431134262516374, 'Place_of_birth_missing_(living_people)'),\n",
       " (0.4331479705370126, 'English_footballers'),\n",
       " (0.49705223728207354, 'Asteroids_named_for_people'),\n",
       " (0.45289685369664195, 'English-language_albums'),\n",
       " (0.44255455664056176, 'Major_League_Baseball_pitchers'),\n",
       " (0.43630320779825527, 'Association_football_midfielders'),\n",
       " (0.45469681783174376,\n",
       "  'Members_of_the_United_Kingdom_Parliament_for_English_constituencies'),\n",
       " (0.3951740593136729, 'British_films'),\n",
       " (0.45864339616958516, 'Indian_films'),\n",
       " (0.4391107123616133, 'Year_of_death_missing'),\n",
       " (0.4429989815967973, 'Association_football_defenders'),\n",
       " (0.39819989403016337, 'American_Jews'),\n",
       " (0.45545292700678036, 'English_cricketers'),\n",
       " (0.43862634487184377, 'Debut_albums'),\n",
       " (0.44159594780243366, 'Association_football_forwards'),\n",
       " (0.43909758020639655, 'Year_of_birth_missing'),\n",
       " (0.45471988736264746, 'Year_of_birth_unknown'),\n",
       " (0.4457018771523092, 'Rivers_of_Romania'),\n",
       " (0.5293290797118242, 'Main_Belt_asteroids'),\n",
       " (0.45313635012889425, 'Windows_games')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(ranking)\n",
    "block_ranking=[]\n",
    "for  i in range(n):\n",
    "    block_ranking.append(heapq.heappop(ranking))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.350766055217393, 'Living_people'),\n",
       " (0.3595978887745843, 'English-language_films'),\n",
       " (0.3688198550787606, 'American_films'),\n",
       " (0.37091218182629093, 'American_film_actors'),\n",
       " (0.3728531191284263, 'American_television_actors'),\n",
       " (0.3732793895257392, 'Black-and-white_films'),\n",
       " (0.3786004143060232, 'Harvard_University_alumni'),\n",
       " (0.38744280712432094, 'People_from_New_York_City'),\n",
       " (0.3882433810587469, 'American_military_personnel_of_World_War_II'),\n",
       " (0.3919424891362564, 'Fellows_of_the_Royal_Society'),\n",
       " (0.3951740593136729, 'British_films'),\n",
       " (0.39593849257133024, 'Article_Feedback_Pilot'),\n",
       " (0.39819989403016337, 'American_Jews'),\n",
       " (0.4000412099261835, 'Year_of_birth_missing_(living_people)'),\n",
       " (0.40389049910919095, 'English_television_actors'),\n",
       " (0.43284885756703656, 'The_Football_League_players'),\n",
       " (0.4331479705370126, 'English_footballers'),\n",
       " (0.43630320779825527, 'Association_football_midfielders'),\n",
       " (0.43862634487184377, 'Debut_albums'),\n",
       " (0.43909758020639655, 'Year_of_birth_missing'),\n",
       " (0.4391107123616133, 'Year_of_death_missing'),\n",
       " (0.44159594780243366, 'Association_football_forwards'),\n",
       " (0.44255455664056176, 'Major_League_Baseball_pitchers'),\n",
       " (0.4429989815967973, 'Association_football_defenders'),\n",
       " (0.4431134262516374, 'Place_of_birth_missing_(living_people)'),\n",
       " (0.4457018771523092, 'Rivers_of_Romania'),\n",
       " (0.4482138406654756, 'Association_football_goalkeepers'),\n",
       " (0.45289685369664195, 'English-language_albums'),\n",
       " (0.45313635012889425, 'Windows_games'),\n",
       " (0.45469681783174376,\n",
       "  'Members_of_the_United_Kingdom_Parliament_for_English_constituencies'),\n",
       " (0.45471988736264746, 'Year_of_birth_unknown'),\n",
       " (0.45545292700678036, 'English_cricketers'),\n",
       " (0.45864339616958516, 'Indian_films'),\n",
       " (0.49705223728207354, 'Asteroids_named_for_people'),\n",
       " (0.5293290797118242, 'Main_Belt_asteroids')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
